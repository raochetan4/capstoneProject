lwd=3,col="blue")
abline(mean(y)-mean(x)*sd(y)/sd(x),
sd(y)/sd(x),
lwd=2)
points(mean(x),mean(y),cex=2,pch=19)
plot(y,x)
b1 <- cor(x,y)*sd(y)/sd(x)
b0 <- mean(y) - b1 * mean(x)
b1
b0
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
yhat <- beta0 + beta1 * x
e <- y - yhat        # residuals
sigma <- sqrt(sum(e^2) / (n - 2))
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
pBeta0
pBeta1
sigma
rm(list=ls()) # clean environment
data(mtcars)
fit <- lm(mtcars$mpg ~ mtcars$wt)
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1,1) * qt(.975, df = fit$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1,1) * qt(.975, df = fit$df) * sumCoef[2, 2]
rm(list=ls()) # clean environment
fit <- lm(mpg ~ wt, data = mtcars)
predict(fit, data.frame(wt = 3), interval="prediction")
rm(list=ls()) # clean environment
fit <- lm(mpg ~ I(wt / 2), data = mtcars)
coef <- summary(fit)$coefficients
mean <- coef[2,1]
stderr <- coef[2,2]
df <- fit$df
#Two sides T-Tests
mean + c(-1,1) * qt(0.975, df = df) * stderr
rm(list=ls()) # clean environment
x <- mtcars$wt
y <- mtcars$mpg
fit <- lm(y ~ x)
c <- 5 # some constant
fit2 <- lm(y ~ I(x + c))
beta0 <- c(fit$coefficients[1], fit2$coefficients[1])
beta1 <- c(fit$coefficients[2], fit2$coefficients[2])
beta0; beta1
# We find that the slope is the same, but that the intercept changed.
# Next we find that:
all.equal(beta0[2], beta0[1] - c * beta1[1]) # TRUE
rm(list=ls()) # clean environment
y <- mtcars$mpg; x <- mtcars$wt
fitWithIntercept <- lm(y ~ x)
yhat1 <- fit$coefficients[1] + x
se1 <- sum((y - yhat1)^2)
yhat2 <- fit$coefficients[1] + fit$coefficients[2] * x
se2 <- sum((y - yhat2)^2)
ratio <- se2 / se1
rm(list=ls()) # clean environment
y <- mtcars$mpg; x <- mtcars$wt
fitWithIntercept <- lm(y ~ x)
yhat1 <- fit$coefficients[1] + x
se1 <- sum((y - yhat1)^2)
yhat2 <- fit$coefficients[1] + fit$coefficients[2] * x
se2 <- sum((y - yhat2)^2)
ratio <- se2 / se1
data(mtcars)
rm(list=ls()) # clean environment
y <- mtcars$mpg; x <- mtcars$wt
fitWithIntercept <- lm(y ~ x)
yhat1 <- fit$coefficients[1] + x
se1 <- sum((y - yhat1)^2)
yhat2 <- fit$coefficients[1] + fit$coefficients[2] * x
se2 <- sum((y - yhat2)^2)
ratio <- se2 / se1
x <- mtcars$wt
y <- mtcars$mpg
fit <- lm(y ~ x)
y <- mtcars$mpg; x <- mtcars$wt
fitWithIntercept <- lm(y ~ x)
yhat1 <- fit$coefficients[1] + x
se1 <- sum((y - yhat1)^2)
yhat2 <- fit$coefficients[1] + fit$coefficients[2] * x
se2 <- sum((y - yhat2)^2)
ratio <- se2 / se1
ratio
y <- mtcars$mpg; x <- mtcars$wt
fit <- lm(y ~ x)
yhat1 <- fit$coefficients[1] + x
se1 <- sum((y - yhat1)^2)
yhat2 <- fit$coefficients[1] + fit$coefficients[2] * x
se2 <- sum((y - yhat2)^2)
ratio <- se2 / se1
ratio
install.package('caret')
install.packages('caret')
install.packages('pbkrtest')
clc
install.packages('ISLR')
library(caret)
library(karet)
library(kernlab)
install.packages('kerlab')
install.packages('kernlab')
data(spam)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type,p=0.75,list=FALSE)
library(caret)
library('caret')
install.packages('pbkrtest')
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: pressure
plot(pressure)
trainData <- read.csv(file="C:\\coursera\\machineLearning\\data\\pml-training.csv", header=TRUE, sep=",")
testData <- read.csv(file="C:\\coursera\\machineLearning\\data\\pml-testing.csv", header=TRUE, sep=",")
head(trainData)
head(trainData$classe)
head(trainData$raw_timestamp_part_1)
head(trainData$raw_timestamp_part_2)
head(trainData$X)
head(trainData$new_window)
plot(trainData$cvtd_timestamp,trainData$classe)
plot(trainData$classe,trainData$cvtd_timestamp)
trainData1=na.omit(train.Data)
trainData1=na.omit(trainData)
head(trainData)
head(trainData1)
trainData(newWindow)
trainData$newWindow
trainData$new_window
trainData$X
trainUrl<-'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
read.csv(url(trainUrl))
trainData<-read.csv(url(trainUrl))
head(trainData)
trainData(trainData==)
trainData(trainData=='#Div/0!')
trainData <- read.csv(url(trainUrl), header=TRUE, sep=",",na.strings=c(NA,"#DIV/0!",""))
testData <- read.csv(url(testUrl), header=TRUE, sep=",",na.strings=c(NA,"#DIV/0!",""))
trainUrl<-'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
testUrl<-'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
trainData <- read.csv(url(trainUrl), header=TRUE, sep=",",na.strings=c(NA,"#DIV/0!",""))
testData <- read.csv(url(testUrl), header=TRUE, sep=",",na.strings=c(NA,"#DIV/0!",""))
nearZeroVar(trainData,saveMetrics=TRUE)
dim(trainData)
plot(trainData$accel_arm_x,trainData$classe)
plot(trainData$classe,trainData$accel_arm_x)
plot(trainData$accel_arm_x,trainData$classe)
trainData$classe
plot(trainData$accel_arm_x,trainData$classe)
qplot(trainData$accel_arm_x,trainData$classe)
library(ggplot2)
qplot(trainData$accel_arm_x,trainData$classe)
boxplot(trainData$accel_arm_x,trainData$classe)
boxplot(trainData$classe,trainData$accel_arm_x)
plot_ly(trainData,x=accel_arm_x,y=classe,group=classe,yaxis=paste0("y",classe),mode='markers')
library(plotly)
install.packages(plotl)
install.packages("plotly")
install.packages("plotly")
plot_ly(trainData,x=accel_arm_x,y=classe,group=classe,yaxis=paste0("y",classe),mode='markers')
library(plotly)
plot_ly(trainData,x=accel_arm_x,y=classe,group=classe,yaxis=paste0("y",classe),mode='markers')
library(caret)
trainData<-trainData[,-(1:7)]
testData<-testData[,-(1:7)]
# The number of NAs if more than half the number of rows we will ignore those columns
useful <- !apply(trainData, 2, function(x) sum(is.na(x)) > 0.5*length(trainData[,1])  || sum(x=="") >  0.5*length(trainData[,1]))
trainData<-trainData[,useful]
testData <-testData[,useful]
#Find all the columns with zero variance. These columns will not have a lot of information and hence we will remove them.
zeroVarColumns <- nearZeroVar(trainData, saveMetrics = TRUE)
trainData <- trainData[,zeroVarColumns$nzv==FALSE]
testData <-testData[,zeroVarColumns$nzv==FALSE]
# Also create a factore variable for classe
trainData$classe <- factor(trainData$classe)
testData$classe <- NA
#Dimentionality Reduction and Use 10 prinComp for training the model
preProc <- preProcess(trainData[,-46],method="pca",prComp=10)
prinComponents<-predict(preProc,trainData[,-46])
prinComponents$classe <- trainData$classe
#Now use the principal Components to  as the predictor variables to train the model.
set.seed(1234)
#Now use the principal Components to  as the predictor variables to train the model.
inTrain<-createDataPartition(trainData$classe,p=3/4,list=FALSE)
trainingSubset<-prinComponents[inTrain,]
validationSet<-prinComponents[-inTrain,]
inTrain<-createDataPartition(validationSet$classe,p=3/4,list=FALSE)
ValidationSetForTest <- validationSet[-inTrain,]
validationSet <- validationSet[inTrain,]
model2 <- train(classe~., data=trainingSubset, method="pls")
model3 <- train(classe~., data=trainingSubset, method="lda")
predict2_pls <- predict(model2,validationSet)
predict3_lda <- predict(model3,validationSet)
head(predict2_pls)
confusionMatrix(predict2_pls,validationSet$classe)
confusionMatrix(predict3_lda,validationSet$classe)
preProc <- preProcess(trainData[,-46],method="pca")
prinComponents<-predict(preProc,trainData[,-46])
prinComponents$classe <- trainData$classe
set.seed(1234)
#Now use the principal Components to  as the predictor variables to train the model.
inTrain<-createDataPartition(trainData$classe,p=3/4,list=FALSE)
trainingSubset<-prinComponents[inTrain,]
validationSet<-prinComponents[-inTrain,]
inTrain<-createDataPartition(validationSet$classe,p=3/4,list=FALSE)
ValidationSetForTest <- validationSet[-inTrain,]
validationSet <- validationSet[inTrain,]
model2 <- train(classe~., data=trainingSubset, method="pls")
model3 <- train(classe~., data=trainingSubset, method="lda")
predict2_pls <- predict(model2,validationSet)
predict3_lda <- predict(model3,validationSet)
confusionMatrix(predict2_pls,validationSet$classe)
confusionMatrix(predict3_lda,validationSet$classe)
preProc <- preProcess(trainData[,-46],method="pca",prComp=10)
prinComponents<-predict(preProc,trainData[,-46])
prinComponents$classe <- trainData$classe
set.seed(1234)
#Now use the principal Components to  as the predictor variables to train the model.
inTrain<-createDataPartition(trainData$classe,p=3/4,list=FALSE)
trainingSubset<-prinComponents[inTrain,]
validationSet<-prinComponents[-inTrain,]
inTrain<-createDataPartition(validationSet$classe,p=3/4,list=FALSE)
ValidationSetForTest <- validationSet[-inTrain,]
validationSet <- validationSet[inTrain,]
model2 <- train(classe~., data=trainingSubset, method="pls")
model3 <- train(classe~., data=trainingSubset, method="lda")
trainData <- read.csv(file="C:\\coursera\\machineLearning\\data\\pml-training.csv", header=TRUE, sep=",")
testData <- read.csv(file="C:\\coursera\\machineLearning\\data\\pml-testing.csv", header=TRUE, sep=",")
library(caret)
trainData<-trainData[,-(1:7)]
testData<-testData[,-(1:7)]
# The number of NAs if more than half the number of rows we will ignore those columns
useful <- !apply(trainData, 2, function(x) sum(is.na(x)) > 0.5*length(trainData[,1])  || sum(x=="") >  0.5*length(trainData[,1]))
trainData<-trainData[,useful]
testData <-testData[,useful]
#Find all the columns with zero variance. These columns will not have a lot of information and hence we will remove them.
zeroVarColumns <- nearZeroVar(trainData, saveMetrics = TRUE)
trainData <- trainData[,zeroVarColumns$nzv==FALSE]
testData <-testData[,zeroVarColumns$nzv==FALSE]
# Also create a factore variable for classe
trainData$classe <- factor(trainData$classe)
testData$classe <- NA
#Dimentionality Reduction and Use 10 prinComp for training the model
preProc <- preProcess(trainData[,-53],method="pca",pcaComp=12)
prinComponents<-predict(preProc,trainData[,-53])
prinComponents$classe <- trainData$classe
# Apply same steps to testData
test_preProc <- preProcess(testData[,-(53:54)],method="pca",pcaComp=12)
test_prinComponents<-predict(test_preProc,testData[,-(53:54)])
test_prinComponents$classe <- testData$classe
set.seed(1234)
#Now use the principal Components to  as the predictor variables to train the model.
inTrain<-createDataPartition(trainData$classe,p=3/4,list=FALSE)
trainingSubset<-prinComponents[inTrain,]
validationSet<-prinComponents[-inTrain,]
inTrain<-createDataPartition(validationSet$classe,p=3/4,list=FALSE)
ValidationSetForTest <- validationSet[-inTrain,]
validationSet <- validationSet[inTrain,]
model4 <- train(classe~., data=trainingSubset, method="rf")
save.image("C:/coursera/machineLearning/model4rf.RData")
predict4_rf <- predict(model4,validationSet)
confusionMatrix(predict4_rf,validationSet$classe)
predict4_rf <- predict(model4,ValidationSetForTest)
confusionMatrix(predict4_rf,ValidationSetForTest$classe)
test_prinComponents$classe <- predict(model4,test_prinComponents[,-13])
cbind(testData[,53],test_prinComponents$classe)
cbind(testData[,53],list(test_prinComponents$classe)
)
cbind(testData[,53],test_prinComponents$classe)
trainData$classe
test_prinComponents<-predict(preProc,testData[,-(53:54)])
test_prinComponents$classe <- testData$classe
test_prinComponents$classe <- predict(model4,test_prinComponents[,-13])
cbind(testData[,53],test_prinComponents$classe)
load("C:/coursera/machineLearning/model1gbm.RData")
load("C:/coursera/machineLearning/modelonegbm.RData")
load("C:/coursera/machineLearning/model1gbm.RData")
load("C:/coursera/machineLearning/model2pls_3lda.RData")
load("C:/coursera/machineLearning/model1gbm.RData")
load("C:/coursera/machineLearning/model4rf.RData")
test_prinComponents<-predict(preProc,testData[,-(53:54)])
test_prinComponents$classe <- testData$classe
save.image("C:/coursera/machineLearning/model4rf.RData")
prComp <- prcomp(trainData[,-53])
plot(cumsum(prComp$sdev^2/sum(prComp$sdev^2)), xlab = "Variables", ylab="Cumulative Var")
Xax<-1:length(trueClass)
plot(Xax,trueClass)
points(Xax, predict1_gbm)
points(Xax, predict2_pls)
points(Xax, predict3_lda)
points(Xax, predict4_rf)
trueClass<-ValidationSetForTest$classe
Xax<-1:length(trueClass)
plot(Xax,trueClass)
points(Xax, predict1_gbm)
points(Xax, predict2_pls)
points(Xax, predict3_lda)
points(Xax, predict4_rf)
load("C:/coursera/machineLearning/model1gbm.RData")
Xax<-1:length(trueClass)
plot(Xax,trueClass)
points(Xax, predict1_gbm)
??points
points(Xax, predict1_gbm,col(2))
points(Xax, predict1_gbm,col("red"))
points(Xax, predict1_gbm,col="red")
line(Xax, predict1_gbm,col="red")
lines(Xax, predict1_gbm,col="red")
Xax<-1:length(trueClass)
plot(Xax,trueClass)
lines(Xax, predict1_gbm,col="red")
plot(cbind(Xax,Xax,Xax,Xax,Xax),df)
plot(cbind(Xax,Xax,Xax,Xax,Xax),cbind(predict1_gbm,predict2_pls,predict3_lda,predict4_rf,trueClass))
plot(Xax,trueClass,type='o')
plot(Xax,trueClass)
lines(predict1_gbm, type="o", pch=22, lty=2, col="red")
lines(predict1_gbm, type="o", pch=22, lty=2, col="red")
lines(predict1_gbm, type="o", pch=22, lty=2, col="red")
plot(Xax,trueClass,col='white')
lines(predict1_gbm, type="o", pch=22, lty=2, col="red")
lines(trueClass, type="o", pch=22, lty=2, col="black")
plot(1, col.axis = "sky blue", col.lab = "thistle")
title("Main Title", sub = "sub title",
cex.main = 2,   font.main= 4, col.main= "blue",
cex.sub = 0.75, font.sub = 3, col.sub = "red")
plot(1, col.axis = "sky blue", col.lab = "thistle")
title("Main Title", sub = "sub title",
cex.main = 0.5,   font.main= 4, col.main= "blue",
cex.sub = 0.75, font.sub = 3, col.sub = "red")
list(test_prinComponents$classe)
test_prinComponents$classe
test_prinComponents$classe <- predict(model4,test_prinComponents[,-13])
# Results for the 20 test Cases
cbind(testData[,53],test_prinComponents$classe)
test_prinComponents<-predict(preProc,testData[,-(53:54)])
test_prinComponents$classe <- testData$classe
test_prinComponents$classe <- predict(model4,test_prinComponents[,-13])
# Results for the 20 test Cases
cbind(testData[,53],test_prinComponents$classe)
test_prinComponents$classe <- predict(model4,test_prinComponents[,-13])
# Results for the 20 test Cases
cbind(testData[,53],list(test_prinComponents$classe))
list(test_prinComponents$classe)
cbind(testData$problem_id,list(test_prinComponents$classe))
(testData$problem_id,list(test_prinComponents$classe))
\testData$problem_id,list(test_prinComponents$classe)
testData$problem_id,list(test_prinComponents$classe)
testData$problem_id, list(test_prinComponents$classe)
testData$problem_id list(test_prinComponents$classe)
testData$problem_id test_prinComponents$classe
cbind(testData$problem_id, test_prinComponents$classe)
ss<-cbind(testData$problem_id, test_prinComponents$classe)
ss
list(ss)
list(ss[,2])
list(test_prinComponents$classe)
ss<-list(test_prinComponents$classe)
ss
ss[[1]]
cbind(testData$problem_id,list(test_prinComponents$classe)[[1]])
ss <- cbind(testData$problem_id,list(test_prinComponents$classe)[[1]])
ss
list(ss)
ss <- cbind(testData$problem_id,as.character(test_prinComponents$classe))
ss
cbind(testData$problem_id,as.vector(test_prinComponents$classe))
cbind(testData$problem_id,as.list(test_prinComponents$classe))
install.packages('devtools')
devtools::install_github('rstudio/rsconnect')
.libPaths()[1]
library(plyr)
data(nasa)
data('nasa')
install.packages(nasa)
install.packages('nasa')
library('nasa')
library(nasa)
library(rsconnect)
rsconnect::deployApp('C:\coursera\DevelopingDataProducts\assignments\examples\CaSolarShinyApp')
library(rsconnect)
rsconnect::deployApp("C:\coursera\DevelopingDataProducts\assignments\examples\CaSolarShinyApp")
library(rsconnect)
rsconnect::deployApp("C:\\coursera\\DevelopingDataProducts\\assignments\\examples\\CaSolarShinyApp")
install.packages('shiny')
library(rsconnect)
rsconnect::deployApp("C:\\coursera\\DevelopingDataProducts\\assignments\\examples\\CaSolarShinyApp")
install.packages("devtools")
library(devtools)
devtools::install_github("twitter/AnomalyDetection")
rawData <- read.table("C:/coursera/capstoneProject/rawData.RData", quote="\"")
View(rawData)
rawData <- read.table("C:/coursera/capstoneProject/rawData.RData", quote="\"")
View(rawData)
clear all
load("C:/coursera/capstoneProject/rawData.RData")
options(mc.cores=4)
library(RWeka)
library(wordcloud)
cleantext<-data.frame(text=unlist(sapply(data, `[`, "content")), stringsAsFactors=F)
onetoken <- NGramTokenizer(cleantext, Weka_control(min = 1, max = 1))
one <- data.frame(table(onetoken))
onesorted <- one[order(one$Freq,decreasing = TRUE),]
one20 <- onesorted[1:20,]
colnames(one20) <- c("Word","Frequency")
ggplot(one20, aes(x=Word,y=Frequency), ) + geom_bar(stat="Identity", fill=cut) +geom_text(aes(label=Frequency), vjust=-0.2)
library(ggplot2)
ggplot(one20, aes(x=Word,y=Frequency), ) + geom_bar(stat="Identity", fill=cut) +geom_text(aes(label=Frequency), vjust=-0.2)
one20
hist(one20)
hist(one20[,2])
hist(one20[,1])
hist(onesorted)
hist(one)
hist(onetoken)
onetoken
hist(onetoken)
??hist
ggplot(one20, aes(x=Word,y=Frequency), ) + geom_bar(stat="Identity", fill=cut) +geom_text(aes(label=Frequency), vjust=-0.2)
plot(one20[,1],one20[,2])
one20[,1]
one20
head(one20)
ggplot(one20[1:5,1],one20[1:5,2])
ggplot(two20, aes(x=Word,y=Frequency), ) + geom_bar(stat="Identity", fill=cut) +geom_text(aes(label=Frequency), vjust=-0.2) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(one20, aes(x=Word,y=Frequency), ) + geom_bar(stat="Identity", fill=cut) +geom_text(aes(label=Frequency), vjust=-0.2) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
hist(one20)
plot(one20[,2])
axis(1, at=1:20, labels=one20[,1])
bitoken <- NGramTokenizer(cleantext, Weka_control(min = 2, max = 2, delimiters = " \\r\\n\\t.,;:\"()?!"))
two <- data.frame(table(bitoken))
twosorted <- two[order(two$Freq,decreasing = TRUE),]
two20 <- twosorted[1:20,]
colnames(two20) <- c("Word","Frequency")
wordcloud(two20$Word, two20$Freq, max.words=60)
#data <- Corpus( DirSource(englishDir),readerControl = list(reader=readPlain, language ="en"))
plot(one20[,1],one20[,2])
plot(one20[,2])
axis(1, at=1:20, labels=one20[,1])
wordcloud(one20$Word, one20$Freq, max.words=60)
setwd("C:/coursera/capstoneProject/May2017Attempt/capstoneForShinyApp/app")
rsconnect::setAccountInfo(name='raochetan4',token='7B73F4BAB36B7F5AACD7895E3622208D',secret='<SECRET>')
library(rsconnect)
rsconnect::setAccountInfo(name='raochetan4',token='7B73F4BAB36B7F5AACD7895E3622208D',secret='<SECRET>')
rsconnect::setAccountInfo(name='raochetan4',token='7B73F4BAB3B7F5AACD7895E3622208D',secret='<SECRET>')
rsconnect::setAccountInfo(name='raochetan4',
token='92CF0E6801C987ED3DE90AA812381BDF',
secret='<SECRET>')
library(shiny)
runApp
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
readRDS(bigram)
readRDS("bigram.RData")
readRDS("/backup/bigram.RData")
readRDS("./backup/bigram.RData")
load("./backup/bigrams.RData")
load("./backup/bigram.RData")
load("C:/coursera/capstoneProject/May2017Attempt/capstoneForShinyApp/app/backup/bigram.RData")
load("C:/coursera/capstoneProject/May2017Attempt/capstoneForShinyApp/app/quadgram.RData")
quadgrams
head(quadgrams)
load("C:/coursera/capstoneProject/May2017Attempt/capstoneForShinyApp/app/backup/quadgram.RData")
source("./backup/bigram.RData")
source("./backup/bigram.RData")
head("./backup/bigram.RData")
source.with.encoding("./backup/bigram.RData")
source("./backup/bigram.RData")
source("./backup/quadgram.RData")
clear
bigram <- read.table("C:/coursera/capstoneProject/May2017Attempt/capstoneForShinyApp/app/bigram.RData", quote="\"")
View(bigram)
head(bigram)
View(quadgrams)
tail(bigram)
runApp()
runApp()
bigram <- read.table("C:/coursera/capstoneProject/May2017Attempt/capstoneForShinyApp/app/bigram.RData", header=TRUE, quote="\"")
View(bigram)
runApp()
source("unigram.RData")
load("unigram.RData")
runApp()
runApp()
runApp()
?img
runApp()
library(rsconnect)
deployApp()
runApp()
runApp()
runApp()
runApp()
runApp()
